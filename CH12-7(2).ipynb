{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### seq2seq 모형 학습"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 랜덤 시드 설정\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 모형 변수 설정\n",
    "n_batch = 64                            # 배치 크기\n",
    "epochs = 100                            # 학습 epochs\n",
    "latent_dim = 256                        # 단어 인코딩 축소 차원\n",
    "n_max_sample = 10000                    # 학습 시킬 최대 샘플 수\n",
    "data_path = './data/seq2seq/fra.txt'    # 데이터 텍스트 파일 경로"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n 'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)',\n 'Go.\\tBouge !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)',\n 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)',\n 'Hi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)',\n 'Run!\\tCours\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)',\n 'Run!\\tCourez\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)',\n 'Run!\\tPrenez vos jambes à vos cous !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077449 (sacredceltic)',\n 'Run!\\tFile !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077454 (sacredceltic)',\n 'Run!\\tFilez !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077455 (sacredceltic)']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 인풋, 타깃 텍스트 데이터 정리\n",
    "x_txts = [] # 영어 문장 모음\n",
    "y_txts = [] # 한글 문장 모음\n",
    "x_chars_uni = set() # 영어 문장 토큰 모음\n",
    "y_chars_uni = set() # 한글 문장 토큰 모음\n",
    "n_sample = min(n_max_sample, len(lines)-1)\n",
    "\n",
    "for line in lines[:n_sample]:\n",
    "    x_txt, y_txt, _ = line.split('\\t')\n",
    "    y_txt = '\\t' + y_txt + '\\n'\n",
    "    x_txts.append(x_txt)\n",
    "    y_txts.append(y_txt)\n",
    "\n",
    "    for char in x_txt:\n",
    "        if char not in x_chars_uni:\n",
    "            x_chars_uni.add(char)\n",
    "    for char in y_txt:\n",
    "        if char not in y_chars_uni:\n",
    "            y_chars_uni.add(char)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니크 인코더 토큰 글자 수: 71\n",
      "유니크 디코더 토큰 글자 수: 93\n",
      "인코더 문장 내 최대 문자 수: 15\n",
      "디코더 문장 내 최대 문자 수: 59\n"
     ]
    }
   ],
   "source": [
    "# 토큰 정리\n",
    "x_chars_uni = sorted(list(x_chars_uni))\n",
    "y_chars_uni = sorted(list(y_chars_uni))\n",
    "n_encoder_tokens = len(x_chars_uni)\n",
    "n_decoder_tokens = len(y_chars_uni)\n",
    "print('유니크 인코더 토큰 글자 수: %d'%n_encoder_tokens)\n",
    "print('유니크 디코더 토큰 글자 수: %d'%n_decoder_tokens)\n",
    "\n",
    "max_encoder_seq_len = 0\n",
    "for txt in x_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_encoder_seq_len = max(txt_len,\n",
    "                              max_encoder_seq_len)\n",
    "print('인코더 문장 내 최대 문자 수: %d'%max_encoder_seq_len)\n",
    "\n",
    "max_decoder_seq_len = 0\n",
    "for txt in y_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_decoder_seq_len = max(txt_len,\n",
    "                              max_decoder_seq_len)\n",
    "print('디코더 문장 내 최대 문자 수: %d'%max_decoder_seq_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 토큰 인덱스\n",
    "x_token_idx = {}\n",
    "for idx, char in enumerate(x_chars_uni):\n",
    "    x_token_idx[char] = idx\n",
    "\n",
    "y_token_idx = {}\n",
    "for idx, char in enumerate(y_chars_uni):\n",
    "    y_token_idx[char] = idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 영 행렬 만들기\n",
    "encoder_x_data = np.zeros((len(x_txts),\n",
    "                             max_encoder_seq_len,\n",
    "                             n_encoder_tokens),\n",
    "                          dtype='float32')\n",
    "decoder_x_data = np.zeros((len(x_txts),\n",
    "                             max_decoder_seq_len,\n",
    "                             n_decoder_tokens),\n",
    "                          dtype='float32')\n",
    "decoder_y_data = np.zeros((len(x_txts),\n",
    "                             max_decoder_seq_len,\n",
    "                             n_decoder_tokens),\n",
    "                          dtype='float32')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 인풋 데이터 행렬\n",
    "for i, x_txt in enumerate(x_txts):\n",
    "    for t, char in enumerate(x_txt):\n",
    "        encoder_x_data[i, t, x_token_idx[char]] = 1.\n",
    "    encoder_x_data[i, t + 1:, x_token_idx[' ']] = 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 타겟 데이터 행렬\n",
    "for i, y_txt in enumerate(y_txts):\n",
    "    for t, char in enumerate(y_txt):\n",
    "        decoder_x_data[i, t, y_token_idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_y_data[i, t - 1, y_token_idx[char]] = 1.\n",
    "\n",
    "    decoder_x_data[i, t + 1:, y_token_idx[' ']] = 1.\n",
    "    decoder_y_data[i, t:, y_token_idx[' ']] = 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 인코더 모형 생성\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 디코더 모형 생성\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens))\n",
    "decoder = LSTM(latent_dim,\n",
    "                return_sequences=True,\n",
    "                return_state=True)\n",
    "decoder_outs, _, _ = decoder(decoder_inputs,\n",
    "                             initial_state=encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(n_decoder_tokens,\n",
    "                                      activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 93)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 335872      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  358400      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 93)     23901       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 1.1566 - accuracy: 0.7355 - val_loss: 1.0397 - val_accuracy: 0.7086\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 18s 2ms/sample - loss: 0.8282 - accuracy: 0.7755 - val_loss: 0.8350 - val_accuracy: 0.7732\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.6622 - accuracy: 0.8139 - val_loss: 0.7048 - val_accuracy: 0.7978\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.5762 - accuracy: 0.8322 - val_loss: 0.6368 - val_accuracy: 0.8158\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.5273 - accuracy: 0.8457 - val_loss: 0.5961 - val_accuracy: 0.8257\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.4925 - accuracy: 0.8553 - val_loss: 0.5587 - val_accuracy: 0.8377\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.4630 - accuracy: 0.8632 - val_loss: 0.5334 - val_accuracy: 0.8429\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.4394 - accuracy: 0.8697 - val_loss: 0.5191 - val_accuracy: 0.8471\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.4186 - accuracy: 0.8754 - val_loss: 0.5081 - val_accuracy: 0.8512\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.4002 - accuracy: 0.8803 - val_loss: 0.4911 - val_accuracy: 0.8558\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.3834 - accuracy: 0.8848 - val_loss: 0.4821 - val_accuracy: 0.8577\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.3675 - accuracy: 0.8896 - val_loss: 0.4717 - val_accuracy: 0.8611\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.3528 - accuracy: 0.8940 - val_loss: 0.4616 - val_accuracy: 0.8646\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.3385 - accuracy: 0.8981 - val_loss: 0.4635 - val_accuracy: 0.8646\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.3257 - accuracy: 0.9017 - val_loss: 0.4549 - val_accuracy: 0.8675\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.3132 - accuracy: 0.9060 - val_loss: 0.4476 - val_accuracy: 0.8702\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.3019 - accuracy: 0.9093 - val_loss: 0.4521 - val_accuracy: 0.8686\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2909 - accuracy: 0.9123 - val_loss: 0.4432 - val_accuracy: 0.8723\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2800 - accuracy: 0.9158 - val_loss: 0.4465 - val_accuracy: 0.8721\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2701 - accuracy: 0.9186 - val_loss: 0.4446 - val_accuracy: 0.8730\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2606 - accuracy: 0.9215 - val_loss: 0.4444 - val_accuracy: 0.8737\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2514 - accuracy: 0.9243 - val_loss: 0.4440 - val_accuracy: 0.8737\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2426 - accuracy: 0.9266 - val_loss: 0.4464 - val_accuracy: 0.8745\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2343 - accuracy: 0.9290 - val_loss: 0.4465 - val_accuracy: 0.8743\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2264 - accuracy: 0.9313 - val_loss: 0.4479 - val_accuracy: 0.8753\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2192 - accuracy: 0.9335 - val_loss: 0.4514 - val_accuracy: 0.8744\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2119 - accuracy: 0.9355 - val_loss: 0.4559 - val_accuracy: 0.8751\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.2050 - accuracy: 0.9374 - val_loss: 0.4625 - val_accuracy: 0.8748\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1984 - accuracy: 0.9396 - val_loss: 0.4627 - val_accuracy: 0.8746\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1923 - accuracy: 0.9415 - val_loss: 0.4680 - val_accuracy: 0.8754\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1862 - accuracy: 0.9431 - val_loss: 0.4711 - val_accuracy: 0.8757\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1812 - accuracy: 0.9446 - val_loss: 0.4739 - val_accuracy: 0.8747\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1750 - accuracy: 0.9469 - val_loss: 0.4750 - val_accuracy: 0.8759\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1702 - accuracy: 0.9478 - val_loss: 0.4803 - val_accuracy: 0.8759\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1650 - accuracy: 0.9493 - val_loss: 0.4818 - val_accuracy: 0.8757\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.1605 - accuracy: 0.9509 - val_loss: 0.4880 - val_accuracy: 0.8756\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.1559 - accuracy: 0.9525 - val_loss: 0.4965 - val_accuracy: 0.8748\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.1514 - accuracy: 0.9536 - val_loss: 0.5037 - val_accuracy: 0.8746\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1469 - accuracy: 0.9550 - val_loss: 0.5087 - val_accuracy: 0.8745\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1432 - accuracy: 0.9560 - val_loss: 0.5108 - val_accuracy: 0.8749\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1394 - accuracy: 0.9573 - val_loss: 0.5093 - val_accuracy: 0.8755\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1357 - accuracy: 0.9582 - val_loss: 0.5169 - val_accuracy: 0.8761\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.1321 - accuracy: 0.9593 - val_loss: 0.5257 - val_accuracy: 0.8739\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.1288 - accuracy: 0.9600 - val_loss: 0.5315 - val_accuracy: 0.8747\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 21s 3ms/sample - loss: 0.1256 - accuracy: 0.9612 - val_loss: 0.5312 - val_accuracy: 0.8746\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.1222 - accuracy: 0.9620 - val_loss: 0.5366 - val_accuracy: 0.8743\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.1193 - accuracy: 0.9630 - val_loss: 0.5468 - val_accuracy: 0.8735\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.1165 - accuracy: 0.9636 - val_loss: 0.5421 - val_accuracy: 0.8747\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.1138 - accuracy: 0.9644 - val_loss: 0.5547 - val_accuracy: 0.8742\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.1109 - accuracy: 0.9655 - val_loss: 0.5612 - val_accuracy: 0.8742\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.1083 - accuracy: 0.9659 - val_loss: 0.5625 - val_accuracy: 0.8741\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.1060 - accuracy: 0.9667 - val_loss: 0.5640 - val_accuracy: 0.8738\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.1032 - accuracy: 0.9674 - val_loss: 0.5682 - val_accuracy: 0.8737\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.1014 - accuracy: 0.9679 - val_loss: 0.5763 - val_accuracy: 0.8739\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0992 - accuracy: 0.9688 - val_loss: 0.5771 - val_accuracy: 0.8741\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0968 - accuracy: 0.9695 - val_loss: 0.5799 - val_accuracy: 0.8738\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0952 - accuracy: 0.9699 - val_loss: 0.5882 - val_accuracy: 0.8743\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 21s 3ms/sample - loss: 0.0931 - accuracy: 0.9704 - val_loss: 0.5916 - val_accuracy: 0.8732\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0911 - accuracy: 0.9711 - val_loss: 0.5965 - val_accuracy: 0.8733\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0891 - accuracy: 0.9716 - val_loss: 0.6030 - val_accuracy: 0.8725\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 21s 3ms/sample - loss: 0.0875 - accuracy: 0.9719 - val_loss: 0.6054 - val_accuracy: 0.8729\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0857 - accuracy: 0.9725 - val_loss: 0.6133 - val_accuracy: 0.8728\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0842 - accuracy: 0.9730 - val_loss: 0.6176 - val_accuracy: 0.8725\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0825 - accuracy: 0.9735 - val_loss: 0.6211 - val_accuracy: 0.8729\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0809 - accuracy: 0.9740 - val_loss: 0.6229 - val_accuracy: 0.8724\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0794 - accuracy: 0.9741 - val_loss: 0.6264 - val_accuracy: 0.8732\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0782 - accuracy: 0.9744 - val_loss: 0.6337 - val_accuracy: 0.8732\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0766 - accuracy: 0.9751 - val_loss: 0.6366 - val_accuracy: 0.8735\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0753 - accuracy: 0.9754 - val_loss: 0.6383 - val_accuracy: 0.8727\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0741 - accuracy: 0.9757 - val_loss: 0.6406 - val_accuracy: 0.8725\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0728 - accuracy: 0.9761 - val_loss: 0.6481 - val_accuracy: 0.8726\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0716 - accuracy: 0.9763 - val_loss: 0.6522 - val_accuracy: 0.8721\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0703 - accuracy: 0.9766 - val_loss: 0.6560 - val_accuracy: 0.8726\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0693 - accuracy: 0.9769 - val_loss: 0.6657 - val_accuracy: 0.8714\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0681 - accuracy: 0.9774 - val_loss: 0.6660 - val_accuracy: 0.8717\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0671 - accuracy: 0.9776 - val_loss: 0.6701 - val_accuracy: 0.8720\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0659 - accuracy: 0.9779 - val_loss: 0.6788 - val_accuracy: 0.8711\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0650 - accuracy: 0.9781 - val_loss: 0.6809 - val_accuracy: 0.8721\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0636 - accuracy: 0.9787 - val_loss: 0.6763 - val_accuracy: 0.8717\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0626 - accuracy: 0.9788 - val_loss: 0.6857 - val_accuracy: 0.8723\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0621 - accuracy: 0.9789 - val_loss: 0.6833 - val_accuracy: 0.8714\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0611 - accuracy: 0.9792 - val_loss: 0.6929 - val_accuracy: 0.8715\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0601 - accuracy: 0.9796 - val_loss: 0.6997 - val_accuracy: 0.8717\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 21s 3ms/sample - loss: 0.0589 - accuracy: 0.9800 - val_loss: 0.7007 - val_accuracy: 0.8702\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 24s 3ms/sample - loss: 0.0580 - accuracy: 0.9802 - val_loss: 0.7060 - val_accuracy: 0.8700\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0575 - accuracy: 0.9803 - val_loss: 0.7005 - val_accuracy: 0.8714\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 21s 3ms/sample - loss: 0.0566 - accuracy: 0.9806 - val_loss: 0.7088 - val_accuracy: 0.8720\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0558 - accuracy: 0.9809 - val_loss: 0.7120 - val_accuracy: 0.8708\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0553 - accuracy: 0.9808 - val_loss: 0.7154 - val_accuracy: 0.8713\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 20s 3ms/sample - loss: 0.0543 - accuracy: 0.9811 - val_loss: 0.7155 - val_accuracy: 0.8707\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0535 - accuracy: 0.9815 - val_loss: 0.7199 - val_accuracy: 0.8705\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 19s 2ms/sample - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.7230 - val_accuracy: 0.8708\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0523 - accuracy: 0.9817 - val_loss: 0.7271 - val_accuracy: 0.8708\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0515 - accuracy: 0.9821 - val_loss: 0.7236 - val_accuracy: 0.8717\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0508 - accuracy: 0.9821 - val_loss: 0.7336 - val_accuracy: 0.8704\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0504 - accuracy: 0.9822 - val_loss: 0.7374 - val_accuracy: 0.8715\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0495 - accuracy: 0.9824 - val_loss: 0.7401 - val_accuracy: 0.8704\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0492 - accuracy: 0.9827 - val_loss: 0.7407 - val_accuracy: 0.8714\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0485 - accuracy: 0.9829 - val_loss: 0.7442 - val_accuracy: 0.8708\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 20s 2ms/sample - loss: 0.0479 - accuracy: 0.9829 - val_loss: 0.7448 - val_accuracy: 0.8718\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x213845f7ac8>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model.fit([encoder_x_data, decoder_x_data], decoder_y_data,\n",
    "          batch_size=n_batch,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 추론 모형 생성\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h,\n",
    "                         decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 토큰 리버스 인덱스\n",
    "reverse_x_char_idx = {}\n",
    "for char, idx in x_token_idx.items():\n",
    "    reverse_x_char_idx[idx] = char\n",
    "\n",
    "reverse_y_char_idx = {}\n",
    "for char, idx in y_token_idx.items():\n",
    "    reverse_y_char_idx[idx] = char"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 결과값 디코딩\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    y_seq = np.zeros((1,1,n_decoder_tokens))\n",
    "    y_seq[0, 0, y_token_idx['\\t']] = 1.\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [y_seq] + states_value)\n",
    "\n",
    "        # sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = reverse_y_char_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "                len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        y_seq = np.zeros((1,1,n_decoder_tokens))\n",
    "        y_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h,c]\n",
    "    return decoded_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Marche.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Marche.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Marche.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fuyez !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Waouh !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Waouh !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Waouh !\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Baissez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Baissez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Baissez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Hide.\n",
      "Decoded sentence: Cache-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Hide.\n",
      "Decoded sentence: Cache-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attendez.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attendez.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attendez.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attendez.\n",
      "\n",
      "-\n",
      "Input sentence: Begin.\n",
      "Decoded sentence: Commencez.\n",
      "\n",
      "-\n",
      "Input sentence: Begin.\n",
      "Decoded sentence: Commencez.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: J'ai gagné.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Du calme.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Souriez !\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Souriez !\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Souriez !\n",
      "\n",
      "-\n",
      "Input sentence: Sorry?\n",
      "Decoded sentence: Pardon ?\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Achète-le !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Achète-le !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Achète-le !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Achète-le !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Mange-le.\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Mange-le.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Aha !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Aha !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Aha !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_idx in range(100):\n",
    "    x_seq = encoder_x_data[seq_idx:seq_idx + 1]\n",
    "    decoded_sentence = decode_sequence(x_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', x_txts[seq_idx])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "py3_7_6",
   "language": "python",
   "display_name": "python3_7_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}